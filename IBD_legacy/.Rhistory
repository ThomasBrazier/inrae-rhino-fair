ctx$loss = list()
# Iterate over each model and optimizer
for (i in seq_along(self$model_list)) {
opt_name = names(self$optimizers)[i]
model = self$model_list[[i]]
optimizer = self$optimizers[[opt_name]]
# Forward pass
preds = model(ctx$input)
# Compute loss
loss = self$nll_loss(preds, ctx$target)
if (ctx$training) {
# Zero gradients
optimizer$zero_grad()
# Backpropagation
loss$backward()
# Update parameters
optimizer$step()
}
# Detach loss to store it
ctx$loss[[opt_name]] = loss$detach()
}
},
# Negative Log-Likelihood loss for Gaussian predictions
nll_loss = function(preds, target) {
mu_train = preds[1]
sig_train = preds[2]
sig_train_pos = torch_log(1 + torch_exp(sig_train)) + 1e-6
loss = torch_mean(0.5 * torch_log(sig_train_pos) + 0.5 * (torch_square(target - mu_train)/sig_train_pos)) + 1
if (is.nan(loss$item())) {
stop("Loss computation returned NaN. Check inputs!")
}
loss
}
)
fitted = nn_ensemble %>%
setup() %>%
set_hparams (model = model, num_models = 5) %>%
fit(train_dl,
epochs = 4,
valid_data = test_dl)
fitted
fitted$records$metrics$train
fitted$model$model_list
?write.table
?devtools::install_github
library(keras)
library(dplyr)
?filter
?starwars
starwars
starwars %>% filter(height > 100)
starwars %>% filter(haircolor == "blond", height > 100)
starwars %>% filter(hair_color == "blond", height > 100)
starwars %>% filter(hair_color == "blond") %>% filter(height > 100)
starwars %>% select("height", "mass")
starwars %>% select("mass", "height")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
# library(abcneuralnet)
devtools::load_all()
library(ggplot2)
library(torch)
library(keras3)
n_train = 2000 # Number of data points
n_obs = 1000 # Validation size
gen_data_1d = function(n) {
sigma = 1
X = matrix(rnorm(n))
w = 2
b = 8
Y = matrix(X %*% w + b + sigma * rnorm(n))
list(X, Y)
}
c(X, Y) %<-% gen_data_1d(n_train + n_obs)
c(X_train, Y_train) %<-% list(X[1:n_train], Y[1:n_train])
c(X_obs, Y_obs) %<-% list(X[(n_train + 1):(n_train + n_obs)],
Y[(n_train + 1):(n_train + n_obs)])
# Predict Y when X is observed
theta = data.frame(y1 = Y_train)
sumstats = data.frame(x1 = X_train)
observed = data.frame(X1 = X_obs)
# Init an abcnn object with inputs and targets
abc = abcnn$new(theta,
sumstats,
observed,
method = 'concrete dropout',
num_hidden_layers = 3,
num_hidden_dim = 256,
epochs = 30,
l2_weight_decay = 1e-5)
abc$summary()
# Plot simulation distribution and observed data points
# abc$plot_lda()
# Use the fit() method to train the neural network
abc$fit()
# Check the fit of the model
as.numeric(unlist(abc$fitted$records$metrics$train))
as.numeric(unlist(abc$fitted$records$metrics$valid))
abc$eval_metrics
# The dropout rate
abc$dropout_rates # Not yet implemented
train_metric = as.numeric(unlist(abc$fitted$records$metrics$train))
valid_metric = as.numeric(unlist(abc$fitted$records$metrics$valid))
eval = abc$eval_metrics$value
train_eval = data.frame(Epoch = rep(1:length(train_metric), 2),
Metric = c(train_metric, valid_metric),
Mode = c(rep("train", length(train_metric)), rep("validation", length(valid_metric))))
ggplot(train_eval, aes(x = Epoch, y = Metric, color = Mode, fill = Mode)) +
geom_point() +
geom_line() +
xlab("Epoch") + ylab("Loss") +
geom_hline(yintercept = eval) +
theme_bw()
# Init an abcnn object with inputs and targets
abc = abcnn$new(theta,
sumstats,
observed,
method = 'concrete dropout',
num_hidden_layers = 3,
num_hidden_dim = 256,
epochs = 30,
batch_size = 1,
l2_weight_decay = 1e-5)
abc$summary()
# Plot simulation distribution and observed data points
# abc$plot_lda()
# Use the fit() method to train the neural network
abc$fit()
# Init an abcnn object with inputs and targets
abc = abcnn$new(theta,
sumstats,
observed,
method = 'concrete dropout',
num_hidden_layers = 3,
num_hidden_dim = 256,
epochs = 30,
batch_size = 32,
l2_weight_decay = 1e-5)
abc$summary()
# Plot simulation distribution and observed data points
# abc$plot_lda()
# Use the fit() method to train the neural network
abc$fit()
train_metric = as.numeric(unlist(abc$fitted$records$metrics$train))
valid_metric = as.numeric(unlist(abc$fitted$records$metrics$valid))
eval = abc$eval_metrics$value
# Check the fit of the model
as.numeric(unlist(abc$fitted$records$metrics$train))
as.numeric(unlist(abc$fitted$records$metrics$valid))
abc$eval_metrics
# The dropout rate
abc$dropout_rates # Not yet implemented
train_metric = as.numeric(unlist(abc$fitted$records$metrics$train))
valid_metric = as.numeric(unlist(abc$fitted$records$metrics$valid))
eval = abc$eval_metrics$value
# Use the fit() method to train the neural network
abc$fit()
# Init an abcnn object with inputs and targets
abc = abcnn$new(theta,
sumstats,
observed,
method = 'concrete dropout',
num_hidden_layers = 3,
num_hidden_dim = 256,
epochs = 30,
batch_size = 32,
l2_weight_decay = 1e-5)
abc$summary()
# Plot simulation distribution and observed data points
# abc$plot_lda()
# Use the fit() method to train the neural network
abc$fit()
# Check the fit of the model
as.numeric(unlist(abc$fitted$records$metrics$train))
as.numeric(unlist(abc$fitted$records$metrics$valid))
abc$eval_metrics
# The dropout rate
abc$dropout_rates # Not yet implemented
train_metric = as.numeric(unlist(abc$fitted$records$metrics$train))
valid_metric = as.numeric(unlist(abc$fitted$records$metrics$valid))
eval = abc$eval_metrics$value
train_eval = data.frame(Epoch = rep(1:length(train_metric), 2),
Metric = c(train_metric, valid_metric),
Mode = c(rep("train", length(train_metric)), rep("validation", length(valid_metric))))
ggplot(train_eval, aes(x = Epoch, y = Metric, color = Mode, fill = Mode)) +
geom_point() +
geom_line() +
xlab("Epoch") + ylab("Loss") +
geom_hline(yintercept = eval) +
theme_bw()
abc$predict()
exp(10)
log(22026)
log10(22026)
log(22026)
# Return a tidy data frame
head(abc$predictions())
df_predicted = abc$predictions()
df_predicted$ci_overall_upper = df_predicted$Predictive_mean + df_predicted$Overall_uncertainty
df_predicted$ci_overall_lower = df_predicted$Predictive_mean - df_predicted$Overall_uncertainty
df_predicted$ci_e_upper = df_predicted$Predictive_mean + df_predicted$Epistemic_uncertainty
df_predicted$ci_e_lower = df_predicted$Predictive_mean - df_predicted$Epistemic_uncertainty
df_predicted$ci_conformal_upper = df_predicted$Predictive_mean + df_predicted$Overall_conformal_credible_interval
df_predicted$ci_conformal_lower = df_predicted$Predictive_mean - df_predicted$Overall_conformal_credible_interval
df_predicted$ci_conformal_e_upper = df_predicted$Predictive_mean + df_predicted$Epistemic_conformal_credible_interval
df_predicted$ci_conformal_e_lower = df_predicted$Predictive_mean - df_predicted$Epistemic_conformal_credible_interval
df_predicted$x = X_obs
df_predicted$y_true = Y_obs
df_training = data.frame(x = X_train,
y = Y_train)
ggplot(data = df_training, aes(x = x, y = y)) +
geom_point(color = "blue", alpha = 0.3) +
# geom_point(data = df_predicted, aes(x = x, y = y_true), color = "green", alpha = 0.3) +
geom_line(data = df_predicted, aes(x = x, y = Predictive_mean), color = "Red") +
geom_point(data = df_predicted, aes(x = x, y = Predictive_mean), color = "Red") +
facet_wrap(~ Parameter, scales = "free") +
geom_ribbon(data = df_predicted, aes(x = x, y = Predictive_mean, ymin = ci_conformal_e_upper, ymax = ci_conformal_e_lower), alpha = 0.4, fill = "purple") +
geom_ribbon(data = df_predicted, aes(x = x, y = Predictive_mean, ymin = ci_conformal_upper, ymax = ci_conformal_lower), alpha = 0.3, fill = "green") +
geom_ribbon(data = df_predicted, aes(x = x, y = Predictive_mean, ymin = ci_overall_lower, ymax = ci_overall_upper), alpha = 0.3, fill = "red") +
geom_ribbon(data = df_predicted, aes(x = x, y = Predictive_mean, ymin = ci_e_lower, ymax = ci_e_upper), alpha = 0.3, fill = "red") +
theme_bw()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
# library(abcneuralnet)
devtools::load_all()
library(ggplot2)
library(torch)
library(keras3)
n_train = 2000 # Number of data points
n_obs = 1000 # Validation size
gen_data_1d = function(n) {
sigma = 1
X = matrix(rnorm(n))
w = 2
b = 8
Y = matrix(X %*% w + b + sigma * rnorm(n))
list(X, Y)
}
c(X, Y) %<-% gen_data_1d(n_train + n_obs)
c(X_train, Y_train) %<-% list(X[1:n_train], Y[1:n_train])
c(X_obs, Y_obs) %<-% list(X[(n_train + 1):(n_train + n_obs)],
Y[(n_train + 1):(n_train + n_obs)])
# Predict Y when X is observed
theta = data.frame(y1 = Y_train)
sumstats = data.frame(x1 = X_train)
observed = data.frame(X1 = X_obs)
# Init an abcnn object with inputs and targets
abc = abcnn$new(theta,
sumstats,
observed,
method = 'concrete dropout',
num_hidden_layers = 3,
num_hidden_dim = 256,
epochs = 30,
batch_size = 32,
l2_weight_decay = 1e-5)
abc$summary()
# Plot simulation distribution and observed data points
# abc$plot_lda()
# Use the fit() method to train the neural network
abc$fit()
log(-1e25)
log(1e25)
log(0.0001)
log(-1e15)
log(-1e6)
log(abs(-1e25))
-log(abs(-1e25))
sign(-1e25)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
# library(abcneuralnet)
devtools::load_all()
library(ggplot2)
library(torch)
library(keras3)
n_train = 2000 # Number of data points
n_obs = 1000 # Validation size
gen_data_1d = function(n) {
sigma = 1
X = matrix(rnorm(n))
w = 2
b = 8
Y = matrix(X %*% w + b + sigma * rnorm(n))
list(X, Y)
}
c(X, Y) %<-% gen_data_1d(n_train + n_obs)
c(X_train, Y_train) %<-% list(X[1:n_train], Y[1:n_train])
c(X_obs, Y_obs) %<-% list(X[(n_train + 1):(n_train + n_obs)],
Y[(n_train + 1):(n_train + n_obs)])
# Predict Y when X is observed
theta = data.frame(y1 = Y_train)
sumstats = data.frame(x1 = X_train)
observed = data.frame(X1 = X_obs)
# Init an abcnn object with inputs and targets
abc = abcnn$new(theta,
sumstats,
observed,
method = 'concrete dropout',
num_hidden_layers = 3,
num_hidden_dim = 256,
epochs = 30,
batch_size = 32,
l2_weight_decay = 1e-5)
abc$summary()
# Plot simulation distribution and observed data points
# abc$plot_lda()
# Use the fit() method to train the neural network
abc$fit()
# Check the fit of the model
as.numeric(unlist(abc$fitted$records$metrics$train))
as.numeric(unlist(abc$fitted$records$metrics$valid))
abc$eval_metrics
# The dropout rate
abc$dropout_rates # Not yet implemented
train_metric = as.numeric(unlist(abc$fitted$records$metrics$train))
valid_metric = as.numeric(unlist(abc$fitted$records$metrics$valid))
eval = abc$eval_metrics$value
train_eval = data.frame(Epoch = rep(1:length(train_metric), 2),
Metric = c(train_metric, valid_metric),
Mode = c(rep("train", length(train_metric)), rep("validation", length(valid_metric))))
ggplot(train_eval, aes(x = Epoch, y = Metric, color = Mode, fill = Mode)) +
geom_point() +
geom_line() +
xlab("Epoch") + ylab("Loss") +
geom_hline(yintercept = eval) +
theme_bw()
abc$plot_training()
save_abcnn(abc, prefix = "../inst/extdata/abc_concrete")
abc = load_abcnn(prefix = "../inst/extdata/abc_concrete")
abc$predict()
hist(as.numeric(abc$fitted$model$parameters$concrete_dropout.conc_drop1.linear.weight))
hist(as.numeric(abc$fitted$model$parameters$concrete_dropout.conc_drop2.linear.weight))
hist(as.numeric(abc$fitted$model$parameters$concrete_dropout.conc_drop3.linear.weight))
# Parameters of simulated input x
data_range = 7
data_step = 0.0005
# Boundaries of the gap in the data range
bound1 = -2
bound2 = 2
# Random noise applied on y
data_sigma1a = 0.1
data_sigma2a = 0.5
data_sigma1b = 0.2
data_sigma2b = 0.1
# Number of simulated data points
# num_data = 10000
# Simulate x1
data_x1a = seq(-data_range, bound1 + data_step, by = data_step)
data_x1b = seq(bound2, data_range + data_step, by = data_step)
# Simulate targets y
data_y1a = sin(data_x1a) + rnorm(length(data_x1a), 0, data_sigma1a)
data_y1b = sin(data_x1b) + rnorm(length(data_x1b), 0, data_sigma2a)
# Shift X1 to get X2
data_x2a = data_x1a + 7
data_x2b = data_x1b + 7
# Simulate targets y
data_y2a = cos(data_x2a) + rnorm(length(data_x2a), 0, data_sigma1b)
data_y2b = cos(data_x2b) + rnorm(length(data_x2b), 0, data_sigma2b)
df = data.frame(x1 = c(data_x1a, data_x1b),
x2 = c(data_x2a, data_x2b),
y1 = c(data_y1a, data_y1b),
y2 = c(data_y2a, data_y2b))
# Shuffle data
shuffle_idx = sample(1:(nrow(df)), nrow(df), replace = FALSE)
df_train = df[shuffle_idx,]
# Train/Test datasets
# test_ratio = 0.1
# num_train_data = round(nrow(df) * (1 - test_ratio), digits = 0)
# num_test_data  = nrow(df) - num_train_data
#
# train_x = df[1:num_train_data, c("x1", "x2")]
# train_y = df[1:num_train_data, c("y1", "y2")]
# test_x = df[num_train_data:nrow(df_train), c("x1", "x2")]
# test_y = df[num_train_data:nrow(df_train), c("y1", "y2")]
train_x = df_train[, c("x1", "x2")]
train_y = df_train[, c("y1", "y2")]
# Make a pseudo-obseerved dataset with out of distribution data points
# Simulate x1
data_x1 = seq(-data_range, data_range, length.out = 1000)
# Simulate true targets y
data_y1 = sin(data_x1)
# Shift X1 to get X2
data_x2 = data_x1 + 7
# Simulate targets y
data_y2 = cos(data_x2)
df_observed = data.frame(x1 = data_x1,
x2 = data_x2,
y1 = data_y1,
y2 = data_y2)
observed_x  = df_observed[, c("x1", "x2")]
observed_y  = df_observed[, c("y1", "y2")]
# Plot the simulated data
p1 = ggplot(data = df_train, aes(x = x1, y = y1)) +
geom_point(color = "Blue", alpha = 0.2) +
geom_line(data = df_observed, aes(x = x1, y = y1), color = "red") +
theme_bw()
p2 = ggplot(data = df_train, aes(x = x2, y = y2)) +
geom_point(color = "Green", alpha = 0.2) +
geom_line(data = df_observed, aes(x = x2, y = y2), color = "red") +
theme_bw()
ggpubr::ggarrange(p1, p2, ncol = 2)
# Predict Y when X is observed
theta = data.frame(y = train_y$y1)
sumstats = data.frame(x = train_x$x1)
observed = data.frame(x = observed_x$x1)
abc_ensemble = abcnn$new(theta,
sumstats,
observed,
method = 'deep ensemble',
num_networks = 5,
epochs = 30,
num_hidden_layers = 3,
num_hidden_dim = 500,
batch_size = 256,
epsilon_adversarial = 0.01)
# The perturbation on input
abc_ensemble$epsilon_adversarial * 2 * abc_ensemble$sumstat_sd
# Predict Y when X is observed
theta = data.frame(y = train_y$y1)
sumstats = data.frame(x = train_x$x1)
observed = data.frame(x = observed_x$x1)
abc_ensemble = abcnn$new(theta,
sumstats,
observed,
method = 'deep ensemble',
num_networks = 5,
epochs = 30,
num_hidden_layers = 3,
num_hidden_dim = 500,
batch_size = 256,
epsilon_adversarial = 0.01)
# The perturbation on input
abc_ensemble$epsilon_adversarial * 2 * abc_ensemble$sumstat_sd
abc_ensemble$fit()
abc_ensemble$plot_training()
abc_ensemble$predict()
abc_ensemble$plot_predicted(paired = TRUE, type = "conformal") +
geom_point(data = df_train, aes(x = x1, y = y1), color = "blue", alpha = 0.01)
abc$n_conformal
library(dplyr)
?inner_join
191749 + 192406
192406 - 191749
1386160 - 1384545
17040093 - 17039923
12116435 - 12072804
nchar("GGCCTCCCCCTTAACCAGTTGTGGTGGTCGAAGCACCTCTTTGATAGTTGTGAGAATGGTTC")
49414 + 62
nchr("tatatatatatatatatatatatatatatatatatatatat")
nchar("tatatatatatatatatatatatatatatatatatatatat")
nchar("GTTTCGGCGTCCCGGACTTGGCGTAAAGATTTGGTTTTGTGAAGTGAAAATAGGGGGTCGGGGCCGTGAAACGATCTCTAAATGGAAATTTCCAATCGTCCACATGATCGCCTCCACTTTATCTGCTTGCCCGTCAAATTTCCTGCCAAACAGATGAGC")
nchar("TGGCGTAAAGATTTGGTTTTGTGAAGTGAAAATAGGGGGTCGGGGCCGTGAAACGATCTCTAAATGGAAATTTCCAATCGTCCACATGATCGCCTCCACTTTATCTGCTTGCCCGTCAAATTTCCTGCCAAACAGATGAGCGTAAGGGCTTGATTTGCG")
nchar("TGGCGTAAAGATTTGGTTTTGTGAAGTGAAAATAGGGGGTCGGGGCCGTGAAACGATCTCTAAATGGAAATTTCCAATCGTCCACATGATCGCCTCCACTTTATCTGCTTGCCCGTCAAATTTCCTGCCAAACAGATGAGCGTAAGGGCTTGATTTGCG")
16135 - 15978
16118 - 15960
15960 + 158
15978 + 158
191752 + 41
4878255 + 494
nchar("GCCGGCTCGATATAATTTGGGGCCTAAGGCGATAATTTTAGACGAGGCCTTTTTACATGGGCCCTCTCAATGATGGTTATTTTTGTAGAGTTATGATTTTATTTCAACAAGCCTCTTTTAGTGGGCCCACTCTCTTTATTGGCTTCCTTGTAGTCCACATGTTTTTAGTTGAGTCTTCTAAATAGGAGAGAAAAAGCACTTTTTTTTTTGTTGGTATTAATTATTGTGTTCTTGAGCCTCTAACTATTTATTTTTCTATTGAAGTTAAAGAGTCTCTAAATAAGAGATCATGCTTCTTTTATTTTTTTCAAAACCATTCAGATTATGATTGGGCTTTTTTTTTTTCAAACAAACTTATCTTTGGGGCCTATTCCACTAGAAAAAAATCGTTTCAGATTAAGATTAAGCTTTTCTTTTTGTATTTTTCTTTTCTCAAACAAACCTATTTTTGGGGGCCTTAGACGACTGCCTAACTCGCCTTATGGACA")
4878260 + 488
2138372 + 37583118
4878255 + 494
4878747 - 4878260
logical()
logical(1)
?optim_adam
?torch::optim_adam
torch::optim_adam
?abc
?abc::abc
# clear global environment: remove all variables
rm(list=ls(all=TRUE))
#----------------------
# Loading packages
# SYSTEM
library(rstudioapi)
library(devtools)
library(MASS)
library(fitdistrplus)
# Libraries 'pscl' (zero-inflated functions) and 'extraDistr' could benefit to the model comparison
library(gamlss.dist)
library(ggplot2)
library(hierfstat)
library(adegenet)
library(geosphere)
library(ade4)
library(ggplot2)
library(adegenet)
allel=read.table("uniqueGenotypesWithInfo.txt",h=T)
setwd("~/Academic/inrae-rhino-fair/IBD_legacy")
allel=read.table("uniqueGenotypesWithInfo.txt",h=T)
# Supprimer les individus de la colonie M399 non prise en compte
allel=allel[-which(allel$idcol=="M399" | allel$idcol=="M1079" | allel$idcol=="CXSGT"| allel$idcol=="M1975"  | allel$idcol=="M1979"),]
#allel=allel[-which(allel$idcol=="M399"),]
# enlever les males (femelles sont liees a la colonie beaucoup plus que les males)
allel=allel[-which(allel$sexe=="M"),]
# enlever les juveniles
allel=allel[-which(allel$ageWhenFirstCaptur=="Juv"),]
# Supprimer les individus avec un genotype incomplet
# i.e. individus avec un 0 parmi tous les alleles
#Enlever les individus avec missing data
allel=allel[!apply(allel[,2:13],1,function (x) 0 %in% x),]
# Construction de l'objet genind qui sera transmis pour la Fst
matAlleles=df2genind(allel[,2:13],sep=NULL,ncode=3,ploidy=2,type="codom",pop=as.numeric(allel$idcol))
